# Player Props Test Outputs

This directory contains output files generated by player props testing scripts.

## Purpose

All test output files for player prop predictions and ML model evaluations are stored here to maintain organized project structure.

## Output Files

Test scripts from `tests/player_props/` save their results here:

- **ML Evaluation Results**: Performance metrics for ML models across different prop types
- **Baseline Comparisons**: Comparisons between baseline predictions and ML predictions
- **Backtest Results**: Historical performance analysis of prop predictions
- **Multi-Year Evaluations**: Performance trends across multiple seasons
- **Prop Type Comparisons**: Side-by-side comparisons of all prop types

## File Naming Convention

Output files typically follow these patterns:
- `ML_EVALUATION_RESULTS.md`: Overall ML model evaluation results
- `ML_THRESHOLD_SETTINGS.md`: Threshold configuration and tuning results
- `PROPS_PERFORMANCE_ANALYSIS.md`: Comprehensive performance analysis
- `*_COMPARISON.md`: Comparison reports
- `*_RESULTS.md`: Test results documentation
- `*.txt`: Raw test output logs

## Generated By

These output files are automatically generated by test scripts in:
- `tests/player_props/evaluate_prop_type.py`
- `tests/player_props/compare_all_prop_types.py`
- `tests/player_props/evaluate_ml_model.py`
- `tests/player_props/evaluate_ml_model_multi_year.py`
- `tests/player_props/backtest_props.py`
- `tests/player_props/test_props_performance.py`
- And other testing utilities

## Usage

To generate new output files, run the appropriate test script:

```bash
# Example: Evaluate a specific prop type
python tests/player_props/evaluate_prop_type.py passing_yards

# Example: Compare all prop types
python tests/player_props/compare_all_prop_types.py
```

Results will be saved to this directory automatically.
